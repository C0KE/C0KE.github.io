<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>MLP 神经网络模型-多层感知机-online | Daily Study</title><meta name="author" content="C0KE"><meta name="copyright" content="C0KE"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="MLP 神经网络模型-多层感知机前言神经网络是当前机器学习领域普遍所应用的，例如可利用神经网络进行图像识别、语音识别等，从而将其拓展应用于自动驾驶汽车。它是一种高度并行的信息处理系统，具有很强的自适应学习能力，不依赖于研究对象的数学模型，对被控对象的的系统参数变化及外界干扰有很好的鲁棒性，能处理复杂的多输入、多输出非线性系统，神经网络要解决的基本问题是分类问题 。 我们的第一个神经网络选择 基于">
<meta property="og:type" content="article">
<meta property="og:title" content="MLP 神经网络模型-多层感知机-online">
<meta property="og:url" content="http://example.com/2024/07/10/MLP%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-online/index.html">
<meta property="og:site_name" content="Daily Study">
<meta property="og:description" content="MLP 神经网络模型-多层感知机前言神经网络是当前机器学习领域普遍所应用的，例如可利用神经网络进行图像识别、语音识别等，从而将其拓展应用于自动驾驶汽车。它是一种高度并行的信息处理系统，具有很强的自适应学习能力，不依赖于研究对象的数学模型，对被控对象的的系统参数变化及外界干扰有很好的鲁棒性，能处理复杂的多输入、多输出非线性系统，神经网络要解决的基本问题是分类问题 。 我们的第一个神经网络选择 基于">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/sealed%E7%99%BD%E5%A4%A9.png">
<meta property="article:published_time" content="2024-07-10T04:04:54.272Z">
<meta property="article:modified_time" content="2024-07-10T07:43:40.076Z">
<meta property="article:author" content="C0KE">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/sealed%E7%99%BD%E5%A4%A9.png"><link rel="shortcut icon" href="/img/fa.jpg"><link rel="canonical" href="http://example.com/2024/07/10/MLP%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-online/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'MLP 神经网络模型-多层感知机-online',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-07-10 15:43:40'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/73c31ad1881611ebb6edd017c2d2eca2.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">370</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">53</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 目录</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/sealed%E7%99%BD%E5%A4%A9.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Daily Study"><span class="site-name">Daily Study</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 目录</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">MLP 神经网络模型-多层感知机-online</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-10T04:04:54.272Z" title="发表于 2024-07-10 12:04:54">2024-07-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-10T07:43:40.076Z" title="更新于 2024-07-10 15:43:40">2024-07-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">人工智能开发学习笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="MLP 神经网络模型-多层感知机-online"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="MLP-神经网络模型-多层感知机"><a href="#MLP-神经网络模型-多层感知机" class="headerlink" title="MLP 神经网络模型-多层感知机"></a>MLP 神经网络模型-多层感知机</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>神经网络是当前机器学习领域普遍所应用的，例如可利用神经网络进行图像识别、语音识别等，从而将其拓展应用于自动驾驶汽车。它是一种高度并行的信息处理系统，具有很强的自适应学习能力，不依赖于研究对象的数学模型，对被控对象的的系统参数变化及外界干扰有很好的鲁棒性，能处理复杂的多输入、多输出非线性系统，<strong>神经网络要解决的基本问题是分类问题</strong> 。</p>
<p>我们的第一个神经网络选择 <strong>基于 BP 误差反向传播法的 MLP 神经网络模型</strong> 。</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/v2-78685a273bd758c985c0d6c82068f88f_b-1702219237661-14.webp" alt="动图"></p>
<h2 id="前置导引"><a href="#前置导引" class="headerlink" title="前置导引"></a>前置导引</h2><h3 id="朴素感知机"><a href="#朴素感知机" class="headerlink" title="朴素感知机"></a>朴素感知机</h3><p>感知机是由美国学者 FrankRosenblatt 在 1957 年提出来的。感知机是作为神经网络（深度学习）的起源的算法。因此，学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。 感知机接收多个输入信号，输出一个信号。这里所说的“信号”可以想象成电流或河流那样具备“流动性”的东西。像电流流过导线，向前方输送电子一样，感知机的信号也会形成流，向前方输送信息。但是，和实际的电流不同的是，感知机的信号只有“流&#x2F;不流”（1&#x2F;0）两种取值。这里我们认为 0 对应“不传递信号”， 1 对应“传递信号”。</p>
<blockquote>
<p> 现在是数学时间！</p>
<p>这里有很多的点，其中一部分的位置如图所示，你能找出一条直线，尽可能多的将所有的点（包括还没画出的）分开吗？对了，记得给我他的函数式。<img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/image-20231210220919065.png" alt="Image title"></p>
</blockquote>
<p>让我们假设这条直线的方程是 $f(x,y)&#x3D;ax+bx+c$小学二年级学过的知识告诉我们，对于给出的每一个点 $(x,y)$</p>
<p>代入直线方程后根据结果大小来判断他和直线的位置关系。因此，我们可以把这个过程抽象为下面的感知机：</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/1702219744283.png" alt="img"></p>
<p>我们还可以把整个感知机抽象为下面的数学模型。<br>$$<br>output&#x3D;<br>\begin{cases}<br>Red&amp;f(x,y)&gt; 0\<br>Green&amp;f(x,y)&lt;0<br>\end{cases}<br>$$<br>如你所见，最简单的感知机就是将输入进行处理得到中间结果，再将结果经过非线性函数处理输出。一般的，我们也把上述的模型称为 <strong>神经元</strong> 。</p>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>神经网络的灵感取自于生物上的神经元细胞，如下图所示：</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/1.png" alt="img"></p>
<p>这是人体神经元的基本构成，其中树突主要用于接收其他神经元的信号，轴突用于输出该神经元的信号，数以万计的神经元相互合作，使得我们人类能够进行高级的思考，能够不断地对新事物进行学习。因此，我们就希望仿照人类神经网络的结构，搭建一种人为的神经网络结构，从而使其能够完成一些计算任务，这也是神经网络名字的由来。</p>
<p>神经网络中计算的基本单元是 <strong>神经元</strong> ，一般称作「节点」（node）或者「单元」（unit）。每个节点可以从其他节点接收输入，或者从外部源接收输入，然后计算输出。每个输入都各自的「权重」（weight，即 w），用于调节该输入对输出影响的大小，节点的结构如图所示：</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/3.png" alt="img"></p>
<p>其中 x1, x2 作为该节点的输入，其权重分别为 w1 和 w2。同时，还有配有「偏置 b」（bias）的输入 ，偏置的主要功能是为每一个节点提供可训练的常量值（在节点接收的正常输入以外）。</p>
<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><p>多层感知机（Multilayer Perceptron）缩写为 MLP，也称作前馈神经网络（Feedforward Neural Network）。它是一种基于神经网络的机器学习模型，通过多层非线性变换对输入数据进行高级别的抽象和分类。</p>
<p>在上面的例子中，我们假定了直线的方程式为<br>$$<br>f(x,y) &#x3D; ax+by+c<br>$$<br>但如果是一些复杂的情况，我们可能无法通过简单的数学计算找到对应的函数。但与单层感知机相比，MLP 有多个隐藏层，每个隐藏层由多个神经元组成，每个神经元通过对上一层的输入进行加权和处理，再通过激活函数进行非线性映射。这样我们便可以表达这个抽象的映射关系。</p>
<h3 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h3><p>下图显示了一个最典型的 MLP，它包括包括三层： <strong>输入层、隐层（全连接层）和输出层</strong> （全连接的意思就是：上一层的任何一个神经元与下一层的所有神经元都有连接）。</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/v2-2110a4d62384a277ab700907e73e8721_b-17119844697317.gif" alt="v2-2110a4d62384a277ab700907e73e8721_b"></p>
<p>它的工作分为两部分:</p>
<ul>
<li><strong>前向传播</strong> ：输入数据被馈送到输入层，然后传递到隐藏层，并最终生成输出层的输出。每一层的每一个神经元都会计算其加权输入和非线性激活函数的输出。</li>
<li><strong>反向传播</strong> ：在训练过程中，输出与期望的输出进行比较，产生一个误差值。这个误差随后被反向传播到网络中，权重得到相应的更新。</li>
</ul>
<p>因为参与运算的各元素都是 <strong>矩阵</strong> ，我们可以使用 Numpy 来构造一个三层的 MLP：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">class BPNetwork_Numpy:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.weights = &#123;</span><br><span class="line">            &#x27;fc1&#x27;: np.random.randn(128, 28 * 28),</span><br><span class="line">            &#x27;fc2&#x27;: np.random.randn(64, 128),</span><br><span class="line">            &#x27;fc3&#x27;: np.random.randn(10, 64)</span><br><span class="line">        &#125;</span><br><span class="line">        self.biases = &#123;</span><br><span class="line">            &#x27;fc1&#x27;: np.random.randn(128),</span><br><span class="line">            &#x27;fc2&#x27;: np.random.randn(64),</span><br><span class="line">            &#x27;fc3&#x27;: np.random.randn(10)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<h3 id="为什么需要引入隐藏层？"><a href="#为什么需要引入隐藏层？" class="headerlink" title="为什么需要引入隐藏层？"></a><strong>为什么需要引入隐藏层？</strong></h3><ol>
<li><strong>模型复杂性和表示能力</strong> ：隐藏层增加了网络的容量，使其能够学习并表示更复杂的函数。理论上，一个单隐藏层的神经网络可以近似任何连续函数，但在实践中，增加更多的隐藏层可以更容易（需要更少的神经元）地近似这些函数，并且可以更高效地学习。</li>
<li><strong>层次化特征学习</strong> ：深度网络（有多个隐藏层的网络）可以学习层次化的特征。例如，在图像识别中，第一层可能会学习边缘和纹理，第二层可能会学习更复杂的结构（如形状或模式），以此类推。这种层次化的表示使得网络能够更有效地捕捉数据的内在结构。</li>
<li><strong>非线性堆叠</strong> ：通过引入多个隐藏层并在每个层之后应用非线性激活函数，网络可以表示更复杂的非线性函数。这增加了模型对各种数据分布和关系的灵活性。</li>
<li><strong>降低参数数量</strong> ：与一个非常宽的单隐藏层相比，深度结构可以用较少的总参数来实现同样的模型容量。这可以提高计算效率，并可能减少过拟合的风险。</li>
<li><strong>实际成功案例</strong> ：在许多实际的机器学习任务中，深度神经网络（带有多个隐藏层）已经表现出比浅层网络更好的性能。例如，深度学习模型已经在图像识别、语音识别、机器翻译等任务中取得了突破性的结果。</li>
</ol>
<p>总的来说，隐藏层提供了网络更多的能力和灵活性来学习和逼近复杂的关系和数据结构。当然，引入太多的隐藏层和神经元也可能导致模型的过拟合，因此需要采用适当的正则化策略来平衡模型的容量和训练数据的数量。</p>
<h3 id="计算方式"><a href="#计算方式" class="headerlink" title="计算方式"></a>计算方式</h3><p>让我们把上面的 MLP 细节化：</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/v2-5a81c06789ad5e9323f93c6540263327_r.jpg" alt="img"></p>
<p>在一般情况下，参与运算的各元素都是 <strong>矩阵</strong>，其中的蓝色方块代表着 <strong>激活函数</strong>，绿色方块代表着 <strong>权重 Weight</strong> 和 <strong>偏置 Bias</strong> 。需要注意的是， ***权重和偏置对应上图中不同神经元的连线***，例如 1-&gt; 3 和 1-&gt; 4 的参数不同。详细计算方式如下图及下式：</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/v2-ec822b0f10066f097a074e1ff09167b1_r.jpg" alt="img"></p>
<p><strong>在分类问题中，最后的输出层元素个数一般等于分类数目，最终的结果代表着该输入被这个 MLP 分为某一类的可能性大小，指越大，代表越可能属于该类。为了便于观测，我们通常使用 softmax 函数将将上一层的原始数据进行归一化，转化为一个(0,1)(0,1)之间的数值，这些数值可以被当做概率分布，用来作为多分类的目标预测值。</strong></p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/v2-4452fdaaa04686aa270010f57f4db2aa_r.jpg" alt="img"></p>
<p>同样的，我们可以使用 Numpy 帮我们完成计算（向前传播）部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">       x = x.flatten()</span><br><span class="line">       <span class="variable language_">self</span>.layer1_output = np.dot(<span class="variable language_">self</span>.weights[<span class="string">&#x27;fc1&#x27;</span>], x) + <span class="variable language_">self</span>.biases[<span class="string">&#x27;fc1&#x27;</span>]</span><br><span class="line">       <span class="variable language_">self</span>.layer1_activation = np.maximum(<span class="number">0</span>, <span class="variable language_">self</span>.layer1_output)</span><br><span class="line">       <span class="variable language_">self</span>.layer2_output = np.dot(<span class="variable language_">self</span>.weights[<span class="string">&#x27;fc2&#x27;</span>], <span class="variable language_">self</span>.layer1_activation) + <span class="variable language_">self</span>.biases[<span class="string">&#x27;fc2&#x27;</span>]</span><br><span class="line">       <span class="variable language_">self</span>.layer2_activation = np.maximum(<span class="number">0</span>, <span class="variable language_">self</span>.layer2_output)</span><br><span class="line">       <span class="variable language_">self</span>.layer3_output = np.dot(<span class="variable language_">self</span>.weights[<span class="string">&#x27;fc3&#x27;</span>], <span class="variable language_">self</span>.layer2_activation) + <span class="variable language_">self</span>.biases[<span class="string">&#x27;fc3&#x27;</span>]</span><br><span class="line">       <span class="keyword">return</span> np.softmax(<span class="variable language_">self</span>.layer3_output)</span><br></pre></td></tr></table></figure>

<p>其中，np.dot()函数提供了矩阵乘法，np.maximum()作为我们的激活函数，最后的 np.softmax()用来输出数据转化为概率，方便我们观测结果。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>我们来看下面这幅图片，对于下面这样更偏向于现实的情况，你还能找出一条 <strong>直线</strong>，尽可能多的将所有的点（包括还没画出的）分开吗？</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/1702218719184.png" alt="img"></p>
<p>很明显，直线是无法做到的，也就意味着，<strong>仅有线性函数无法处理复杂问题</strong>。使用激活函数，能够给神经元引入非线性因素，使得神经网络增加了模型的表示能力，能够捕捉到数据中更复杂的模式和关系，这样神经网络就可以利用到更多的现实复杂模型中。</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/image-20231210225600016.png" alt="img"></p>
<p>其次，处理复杂问题时，仅靠一个神经元是远远不够的，我们更多的是使用多层感知机，不妨来看下面的推导：<br>$$<br>f_1(x)&#x3D;a_1x+b_1\<br>f_2(x)&#x3D;a_2f_1(x)+b_2\<br>f_i(x)&#x3D;a_if_{i-1}(x)+b_i<br>$$<br>其中的<em>f</em>(<em>x</em>)均为线性函数，由数学归纳法可得<em>n</em>层感知机的输出<br>$$<br>\begin{cases}<br>Bias&#x3D;\sum_{k&#x3D;1}^{n}\prod_{j&#x3D;1}^{k-1}a_j<em>b_k\<br>Weight &#x3D; \prod_{k&#x3D;1}^{n}a_k\<br>Output &#x3D; Weight</em>x+Bias<br>\end{cases}<br>$$<br>我们可以看到最后的 Output 表达式和一个神经元的别无两样，因此 <strong>如果不使用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。</strong></p>
<h2 id="图片的另一种形态"><a href="#图片的另一种形态" class="headerlink" title="图片的另一种形态"></a>图片的另一种形态</h2><p>了解了什么是 MLP，每一层如何计算，输出是怎么样的，我们再来思考一下输入，如何把一张图片塞到计算机中？</p>
<blockquote>
<p>从 misc 看图片</p>
<p>如果让你向一位视力障碍人来描述一幅图片，你会怎样去描述它？物体和他们的属性、风格、情感、布局如果是想计算机呢，你又该怎么去描述它？</p>
</blockquote>
<p>每一张图片，我们都可以看作是几个二维矩阵的叠加（ARGB-4 个，RGB-3 个，L-1 个），如下图这张黑白图片所示，每个 x，y 所对应的点都具有自己的值，我们通过归一化将他们从 0-255 整型数映射到 0.0-1.0 的浮点数，颜色越淡（白），值越大。这样，我们可以把每一张图片拆为一个 n 维矩阵。通过这样的方式，我们可以把我们的视觉和计算机的数据联系起来，然后通过一些方式进行计算。</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/pixel-values-171198526532520.png" alt="img"></p>
<p>因此对于一张 28 * 28 的灰度图像，我们可以将他* <em>展平 flatten</em> *为一个 28 * 28 &#x3D; 784个元素的矩阵</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/pixels-to-neurons.gif" alt="pixels-to-neurons"></p>
<p>紧接着，用我们上面讲到的知识，将这个矩阵放到 MLP 中进行计算，得到最后的输出，选择最大值的下标作为它的预测结果。下图中亮起的神经元代表某次计算中，输出经过激活函数后激活了。</p>
<h2 id="BP-误差反向传播法"><a href="#BP-误差反向传播法" class="headerlink" title="BP 误差反向传播法"></a>BP 误差反向传播法</h2><p>下图简要的展示了反向传播算法的核心概念，大致来说：<strong>通过比较现实输出与期望输出之间的差异，根据差异来反向更新每一层的参数，从而使现实输出更加贴近于期望输出。</strong></p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/v2-b84fee51daf26d27bd90420311d410a2_r-1701958087743-6-171198572633746.jpg" alt="img"></p>
<h3 id="数学推导过程"><a href="#数学推导过程" class="headerlink" title="数学推导过程"></a>数学推导过程</h3><p>损失对参数梯度的反向传播可以被这样直观解释：由 A 到传播 B，即由$∂L&#x2F;∂A$得到$∂L&#x2F;∂B$，由导数链式法则$∂L&#x2F;∂B&#x3D;(∂L&#x2F;∂A)*(∂A&#x2F;∂B)$。所以神经网络的BP就是通过链式法则求出$L$对所有参数梯度的过程。</p>
<p>如下图示例，输入$x$,经过网络的参数$w,b$,得到一系列中间结果$a,h$。</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/v2-d38c136a7b018fb3ea7344a902d0e3d1_r-17119862306323.jpg" alt="img"><em>a</em> 表示通过权重和偏置的结果，还未经过激活函数， $ℎ$ 表示经过激活函数后的结果。灰色框内表示 **$L$**对各中间计算结果的梯度，这些梯度的反向传播有两类</p>
<ul>
<li>由 $h$到$a$ ，通过激活函数，如右上角</li>
</ul>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234537965.png" alt="image-20240401234537965"></p>
<ul>
<li>由到$a$到$ℎ$ ，通过权重，如橙线部分</li>
</ul>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234624703.png" alt="image-20240401234624703"></p>
<p>​	可以看出梯度的传播和前向传播的模式是一致的，只是方向不同。</p>
<p>​	计算完了灰色框的部分（损失对中间结果$a,h$ 的梯度），损失对参数 $w,b$的梯度也就显而易见了，以图中红色的 $w_1$和$b_{21}$为例：</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/image-20240401234750036.png" alt="image-20240401234750036"></p>
<p>​	因此，我们可以如下图，将反向传播的表达式和代码如下。</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/v2-4564c3328f7f007f2a046076cdac800a_r-17119864831585.jpg" alt="img"></p>
<p>输出层的反向传播略有不同，因为在分类任务中输出层若用到 softmax 激活函数， $a$ 到 $ℎ$ 不是逐个对应的，如下图所示，因此$$∂L&#x2F;∂B&#x3D;(∂L&#x2F;∂A)*(∂A&#x2F;∂B)$$ 中的 element-wise 相乘是失效的，需要用 $∂L&#x2F;∂ℎ$ 乘以向量 $h$ 到向量 $a$的向量梯度（雅可比矩阵）。</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/v2-eee0b51486476bf9fcb8a1c3b041259f_r-17119865761417.jpg" alt="img"></p>
<p>但实际上，<strong>经过看上去复杂的计算后输出层$ ∂L&#x2F;∂ℎ$ 会计算出一个非常简洁的结果: $h-y$</strong></p>
<ul>
<li>以分类任务为例（交叉熵损失、softmax、训练标签 $y$ 为 one-hot 向量其中第$k$维为 1）：</li>
</ul>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/image-20240401235059326.png" alt="image-20240401235059326"></p>
<ul>
<li>以回归任务为例（二次损失、线性激活、训练标签 $y$ 为实数向量）：</li>
</ul>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/image-20240401235128957.png" alt="image-20240401235128957"></p>
<h3 id="底层实现"><a href="#底层实现" class="headerlink" title="底层实现"></a>底层实现</h3><p>同样的，用 numpy 来实现底层的算法，在下面的例子中，我们选择 ReLU 函数作为激活函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">    <span class="comment"># 计算输出层的梯度</span></span><br><span class="line">    one_hot_y = np.zeros_like(<span class="variable language_">self</span>.layer3_output)</span><br><span class="line">    one_hot_y[y] = <span class="number">1</span></span><br><span class="line">    delta3 = <span class="variable language_">self</span>.layer3_output - one_hot_y</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算第二隐藏层的梯度</span></span><br><span class="line">    delta2 = np.dot(<span class="variable language_">self</span>.weights[<span class="string">&#x27;fc3&#x27;</span>].T, delta3)</span><br><span class="line">    delta2[<span class="variable language_">self</span>.layer2_output &lt;= <span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># ReLU的导数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算第一隐藏层的梯度</span></span><br><span class="line">    delta1 = np.dot(<span class="variable language_">self</span>.weights[<span class="string">&#x27;fc2&#x27;</span>].T, delta2)</span><br><span class="line">    delta1[<span class="variable language_">self</span>.layer1_output &lt;= <span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># ReLU的导数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算权重和偏置项的梯度</span></span><br><span class="line">    dW3 = np.outer(delta3, <span class="variable language_">self</span>.layer2_activation)</span><br><span class="line">    db3 = delta3</span><br><span class="line">    dW2 = np.outer(delta2, <span class="variable language_">self</span>.layer1_activation)</span><br><span class="line">    db2 = delta2</span><br><span class="line">    dW1 = np.outer(delta1, x.flatten())</span><br><span class="line">    db1 = delta1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新权重和偏置项</span></span><br><span class="line">    learning_rate = <span class="number">0.01</span></span><br><span class="line">    <span class="variable language_">self</span>.weights[<span class="string">&#x27;fc3&#x27;</span>] -= learning_rate * dW3</span><br><span class="line">    <span class="variable language_">self</span>.biases[<span class="string">&#x27;fc3&#x27;</span>] -= learning_rate * db3</span><br><span class="line">    <span class="variable language_">self</span>.weights[<span class="string">&#x27;fc2&#x27;</span>] -= learning_rate * dW2</span><br><span class="line">    <span class="variable language_">self</span>.biases[<span class="string">&#x27;fc2&#x27;</span>] -= learning_rate * db2</span><br><span class="line">    <span class="variable language_">self</span>.weights[<span class="string">&#x27;fc1&#x27;</span>] -= learning_rate * dW1</span><br><span class="line">    <span class="variable language_">self</span>.biases[<span class="string">&#x27;fc1&#x27;</span>] -= learning_rate * db1</span><br></pre></td></tr></table></figure>

<h2 id="MNIST-数据集"><a href="#MNIST-数据集" class="headerlink" title="MNIST 数据集"></a>MNIST 数据集</h2><blockquote>
<p>为什么这次的例子中输入都是 28∗28&#x3D;78428∗28&#x3D;784 的？为什么是黑白图片，常规的图片不是有三至四个维度吗？</p>
</blockquote>
<p>就像无数人从敲下“Hello World”开始代码之旅一样，许多研究员从“MNIST 数据集”开启了人工智能的探索之路。</p>
<p><strong>MNIST 数据集</strong>（Mixed National Institute of Standards and Technology database）来自美国国家标准与技术研究所，训练集 (training set) 由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员. 测试集(test set) 也是同样比例的手写数字数据。是一个用来训练各种图像处理系统的二进制图像数据集，广泛应用于机器学习中的训练和测试。作为一个入门级的计算机视觉数据集，发布 20 多年来，它已经被无数机器学习入门者“咀嚼”千万遍，是最受欢迎的深度学习数据集之一。</p>
<p><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/Sample-images-of-MNIST-data-17119867486079.png" alt="Sample images of MNIST data. | Download Scientific Diagram"></p>
<p>整个数据集包含两部分，<strong>训练集</strong> 和 <strong>测试集</strong>。训练集 60000 张图像，其中 30000 张来自 NIST 的 Special Database 3，30000 张来自 NIST 的 Special Database 1；测试集 10000 张图像，其中 5000 张来自 NIST 的 Special Database 3，5000 张来自 NIST 的 Special Database 1。</p>
<p>MNIST 原始的 Special Database 3 数据集和 Special Database 1 数据集均是二值图像，MNIST 从这两个数据集中取出图像后，通过图像处理方法使得每张图像都变成 28 × 28 大小的灰度图像，且手写数字在图像中居中显示。</p>
<h2 id="Pytorch-实现与解析"><a href="#Pytorch-实现与解析" class="headerlink" title="Pytorch 实现与解析"></a>Pytorch 实现与解析</h2><p>下面的代码使用 <strong>pytorch</strong> 构造了一个简单的三层（784 → 128 → 64 → 10）MLP，使用了 ReLU 作为激活函数，最后一层通过 softmax 进行归一化，转化为概率分布，我们将依靠 MNIST 这个著名的数据集对它进行训练，并测试我们的模型效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">64</span>     <span class="comment">#设置每一批的大小</span></span><br><span class="line">EPOCHS = <span class="number">5</span>      <span class="comment">#设置训练轮数</span></span><br><span class="line">DEVICE = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)       <span class="comment">#确定是否有可用的CUDA设备</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个BPNet类，它继承于nn.Module类，nn.Module是pytorch给出的默认神经网络类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):     <span class="comment">#定义类对象实例化方法</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()      <span class="comment">#初始化神经网络</span></span><br><span class="line">        <span class="comment">#定义类的接口函数，使用nn.Sequential将操作合并到一起</span></span><br><span class="line">        <span class="variable language_">self</span>.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">128</span>),        <span class="comment">#放入了一个全连接层，从28*28—&gt;128，28*28是图像大小</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),          <span class="comment">#放入了一个ReLU激活函数，inplace=True节省显存</span></span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">64</span>),             <span class="comment">#放入了一个全连接层，从128—&gt;64，128是上层大小</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),          <span class="comment">#放入了一个ReLU激活函数</span></span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)               <span class="comment">#放入了一个全连接层，从64—&gt;10，64是上层大小</span></span><br><span class="line">        )</span><br><span class="line">    <span class="comment">#forward函数定义该神经如何处理数据，也就是数据如何在网络中前进</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)       <span class="comment">#首先将x展平为一维数组</span></span><br><span class="line">        x = <span class="variable language_">self</span>.classifier(x)      <span class="comment">#将x放入上面定义的函数中</span></span><br><span class="line">        x = nn.Softmax(dim=<span class="number">1</span>)(x)           <span class="comment">#将x进行归一化处理，转换为概率分布</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义训练神经网络模型的代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment">#使用pytorch框架给出的接口datasets，创建一个MNIST数据集</span></span><br><span class="line">    train_dataset = datasets.MNIST(root=<span class="string">r&#x27;data&#x27;</span>,        <span class="comment">#root指明文件存储位置</span></span><br><span class="line">                                train=<span class="literal">True</span>,     <span class="comment">#train代表是否是训练集</span></span><br><span class="line">                                transform=transforms.ToTensor(),        <span class="comment">#transform表明数据集的数据应该如何处理</span></span><br><span class="line">                                download=<span class="literal">True</span>)      <span class="comment">#download表明是否运行下载</span></span><br><span class="line">    <span class="comment">#使用pytorch框架给出的接口DataLoader，将数据集载入</span></span><br><span class="line">    train_loader = DataLoader(dataset=train_dataset,        <span class="comment">#dataset指明数据集</span></span><br><span class="line">                            batch_size=BATCH_SIZE,      <span class="comment">#batch_size指明批大小</span></span><br><span class="line">                            shuffle=<span class="literal">True</span>)       <span class="comment">#shuffle指明是否需要打乱</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#实例化一个Adam优化器，用来对数据进行优化</span></span><br><span class="line">    optimizer = Adam(model.parameters(), lr=<span class="number">0.05</span>)       <span class="comment">#第一个参数指明需要被优化的数据是模型的参数，lr指明了学习率</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()       <span class="comment">#指明误差计算方式，这里使用交叉熵损失</span></span><br><span class="line">    model.train()       <span class="comment">#可以先理解为：训练模式，启动！（启用 Batch Normalization 和 Dropout）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#训练EPOCHS这么多轮（话说是不是es</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">        <span class="comment">#使用enumerate对train_loader进行迭代</span></span><br><span class="line">        <span class="keyword">for</span> index, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line"></span><br><span class="line">            data, target = data.to(DEVICE), target.to(DEVICE)       <span class="comment">#将数据和真实标签部署在选定的设备上，加快运算速度</span></span><br><span class="line">            optimizer.zero_grad()       <span class="comment">#每次训练前要先将梯度归零</span></span><br><span class="line"></span><br><span class="line">            pred = model(data)      <span class="comment">#使用现在的模型得到现实输出</span></span><br><span class="line">            loss = criterion(pred, target)      <span class="comment">#计算现实输出和期望输出之间的误差</span></span><br><span class="line"></span><br><span class="line">            loss.backward()     <span class="comment">#反向传播计算得到每个参数的梯度值</span></span><br><span class="line">            optimizer.step()        <span class="comment">#梯度下降执行一步参数更新</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#每到一定阶段就打印目前训练进度以及相关信息，下面代码是print(f&quot;&quot;)格式化输出，看不懂就把代码跑起来一看就懂</span></span><br><span class="line">            <span class="keyword">if</span> index % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;Train Epoch: <span class="subst">&#123;epoch&#125;</span> [<span class="subst">&#123;index * <span class="built_in">len</span>(data)&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(train_loader.dataset)&#125;</span> (<span class="subst">&#123;(<span class="number">100.</span> * index / <span class="built_in">len</span>(train_loader)):<span class="number">.0</span>f&#125;</span>%)]\tLoss: <span class="subst">&#123;loss.item():<span class="number">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()        <span class="comment">#训练结束（不启用 Batch Normalization 和 Dropout）</span></span><br><span class="line">    torch.save(model.state_dict(),<span class="string">&quot;MLP.pth&quot;</span>)        <span class="comment">#保存模型（仅保存参数）</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义校验模型效果的代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="comment">#如果你问我这里为什么没注释我就梆梆给你两拳你自己看看上面写的什么气死我了我再梆梆给你两拳自己往上翻读代码去</span></span><br><span class="line">    test_dataset = datasets.MNIST(root=<span class="string">&#x27;data&#x27;</span>,</span><br><span class="line">                            train=<span class="literal">False</span>,</span><br><span class="line">                            transform=transforms.ToTensor(),</span><br><span class="line">                            download=<span class="literal">True</span>)</span><br><span class="line">    test_loader = DataLoader(dataset=test_dataset,</span><br><span class="line">                            batch_size=BATCH_SIZE,</span><br><span class="line">                            shuffle=<span class="literal">False</span>)</span><br><span class="line">    total = <span class="number">0</span>       <span class="comment">#总计</span></span><br><span class="line">    correct = <span class="number">0</span>     <span class="comment">#正确</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#和训练一样，使用enumerate对test_loader进行迭代</span></span><br><span class="line">    <span class="keyword">for</span> index, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader):</span><br><span class="line">        data, target = data.to(DEVICE), target.to(DEVICE)</span><br><span class="line"></span><br><span class="line">        pred = model(data)</span><br><span class="line">        correct += (torch.argmax(pred, dim=<span class="number">1</span>) == target).<span class="built_in">sum</span>().item()     <span class="comment">#检测预测是否正确，因为是批处理，所以求和</span></span><br><span class="line">        total += pred.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Correct : &quot;</span>,correct,<span class="string">&#x27;/&#x27;</span>,total,sep=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy : &#x27;</span>,<span class="built_in">float</span>(correct)/<span class="built_in">float</span>(total) * <span class="number">100.</span> ,<span class="string">&quot;%&quot;</span>,sep=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#通过main函数调用个函数及方法</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    model = MLP().to(DEVICE)        <span class="comment">#在指定的设备（cpu或者gpu）上将模型实例化</span></span><br><span class="line">    train(model)</span><br><span class="line">    test(model)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">C0KE</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/07/10/MLP%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-online/">http://example.com/2024/07/10/MLP%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-online/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Daily Study</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/sealed%E7%99%BD%E5%A4%A9.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/10/MLP%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="MLP 神经网络模型-多层感知机"><img class="cover" src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/Sci-Fi%20Energy%20Core.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">MLP 神经网络模型-多层感知机</div></div></a></div><div class="next-post pull-right"><a href="/2024/07/10/CUDA%E4%B8%8ECUDNN%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" title="CUDA与CUDNN安装教程"><img class="cover" src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/sealed%E5%A4%9C%E6%99%9A.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CUDA与CUDNN安装教程</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/73c31ad1881611ebb6edd017c2d2eca2.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">C0KE</div><div class="author-info__description">C0KE's Study Diary</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">370</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">53</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/C0KE"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/C0KE" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://blog.csdn.net/weixin_62675330?spm=1000.2115.3001.5343" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="mailto:2269279877@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">人人都有选择如何活着的权力</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#MLP-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.</span> <span class="toc-text">MLP 神经网络模型-多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E7%BD%AE%E5%AF%BC%E5%BC%95"><span class="toc-number">1.2.</span> <span class="toc-text">前置导引</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.2.1.</span> <span class="toc-text">朴素感知机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.2.2.</span> <span class="toc-text">神经网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.3.</span> <span class="toc-text">多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="toc-number">1.3.1.</span> <span class="toc-text">基本结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%BC%95%E5%85%A5%E9%9A%90%E8%97%8F%E5%B1%82%EF%BC%9F"><span class="toc-number">1.3.2.</span> <span class="toc-text">为什么需要引入隐藏层？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F"><span class="toc-number">1.3.3.</span> <span class="toc-text">计算方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.4.</span> <span class="toc-text">激活函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%BD%A2%E6%80%81"><span class="toc-number">1.4.</span> <span class="toc-text">图片的另一种形态</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BP-%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95"><span class="toc-number">1.5.</span> <span class="toc-text">BP 误差反向传播法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B"><span class="toc-number">1.5.1.</span> <span class="toc-text">数学推导过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.5.2.</span> <span class="toc-text">底层实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MNIST-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.6.</span> <span class="toc-text">MNIST 数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch-%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%A7%A3%E6%9E%90"><span class="toc-number">1.7.</span> <span class="toc-text">Pytorch 实现与解析</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/07/10/BP%20%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%20-%20online/" title="BP 误差反向传播法数学知识 - online"><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/Sci-Fi%20Energy%20Core.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="BP 误差反向传播法数学知识 - online"/></a><div class="content"><a class="title" href="/2024/07/10/BP%20%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%20-%20online/" title="BP 误差反向传播法数学知识 - online">BP 误差反向传播法数学知识 - online</a><time datetime="2024-07-10T07:41:47.045Z" title="发表于 2024-07-10 15:41:47">2024-07-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/10/%E7%BA%BF%E7%A8%8B%EF%BC%8C%E8%BF%9B%E7%A8%8B%EF%BC%8C%E5%8D%8F%E7%A8%8B%E7%9A%84%E5%8C%BA%E5%88%AB/" title="线程，进程，协程的区别"><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/Sci-Fi%20Energy%20Core.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="线程，进程，协程的区别"/></a><div class="content"><a class="title" href="/2024/07/10/%E7%BA%BF%E7%A8%8B%EF%BC%8C%E8%BF%9B%E7%A8%8B%EF%BC%8C%E5%8D%8F%E7%A8%8B%E7%9A%84%E5%8C%BA%E5%88%AB/" title="线程，进程，协程的区别">线程，进程，协程的区别</a><time datetime="2024-07-10T04:04:54.315Z" title="发表于 2024-07-10 12:04:54">2024-07-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/10/%E8%BF%9B%E7%A8%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1/" title="进程间的通信"><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/sealed%E5%A4%9C%E6%99%9A.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="进程间的通信"/></a><div class="content"><a class="title" href="/2024/07/10/%E8%BF%9B%E7%A8%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1/" title="进程间的通信">进程间的通信</a><time datetime="2024-07-10T04:04:54.315Z" title="发表于 2024-07-10 12:04:54">2024-07-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/10/linux%E7%AE%A1%E9%81%93/" title="linux管道"><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/sealed%E7%99%BD%E5%A4%A9.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="linux管道"/></a><div class="content"><a class="title" href="/2024/07/10/linux%E7%AE%A1%E9%81%93/" title="linux管道">linux管道</a><time datetime="2024-07-10T04:04:54.314Z" title="发表于 2024-07-10 12:04:54">2024-07-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/10/%E7%BA%BF%E7%A8%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/" title="线程间的通信方式"><img src="https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/sealed%E5%A4%9C%E6%99%9A.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="线程间的通信方式"/></a><div class="content"><a class="title" href="/2024/07/10/%E7%BA%BF%E7%A8%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/" title="线程间的通信方式">线程间的通信方式</a><time datetime="2024-07-10T04:04:54.314Z" title="发表于 2024-07-10 12:04:54">2024-07-10</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://picgo-c0ke.oss-cn-hangzhou.aliyuncs.com/img/sealed%E7%99%BD%E5%A4%A9.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By C0KE</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-show-text.min.js" data-mobile="false" data-text="C0KE,LOVE,YOU" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>